{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare table in raw layer\n",
    "\n",
    "Let us prepare table in raw layer. This will act as golden copy of our source data.\n",
    "* The files in the landing folder are in JSON Format.\n",
    "* There is no partitioning strategy while copying the files in landing zone.\n",
    "* Parquet file is preferred over other file formats while storing the data in Data Lake storage layer (HDFS in this case).\n",
    "* It is generally preferred to use daily partitions for the data to process further in incremental fashion.\n",
    "\n",
    "Here are the steps we are going to follow. For now, we will take care of this manually but we need to automate and orchestrate later.\n",
    "* Make sure files are available in landing zone.\n",
    "* Read the data from the JSON files in the landing zone and create a Dataframe.\n",
    "* Add additional columns as per the partitioning strategy. We are going to partition by year, then month, then day using one of date fields in input data.\n",
    "* Partition the data frame by year, month and day and then write to the target table in the **{username}_ghraw_db**\n",
    "* Make sure data is accessible using Spark SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/${USER}/itvgithub/landing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Analyze GitHub Archive Data'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions', 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dt = '2021-01-13'\n",
    "ghdata = spark. \\\n",
    "    read. \\\n",
    "    json(f'/user/{username}/itv-github/landing/{process_dt}-*.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'CREATE DATABASE IF NOT EXISTS {username}_ghraw_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'DROP TABLE IF EXISTS {username}_ghraw_db.ghactivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/${USER}/warehouse/${USER}_ghraw_db.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -rm -R -skipTrash /user/${USER}/warehouse/${USER}_ghraw_db.db/ghactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghdata.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghdata. \\\n",
    "    withColumn('year', substring('created_at', 1, 4)). \\\n",
    "    withColumn('month', substring('created_at', 6, 2)). \\\n",
    "    withColumn('day', substring('created_at', 9, 2)). \\\n",
    "    select('repo.*', 'actor.*', 'org.*', 'created_at', 'year', 'month', 'day'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghdata = ghdata. \\\n",
    "    withColumn('year', substring('created_at', 1, 4)). \\\n",
    "    withColumn('month', substring('created_at', 6, 2)). \\\n",
    "    withColumn('day', substring('created_at', 9, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghdata. \\\n",
    "    write. \\\n",
    "    partitionBy('year', 'month', 'day'). \\\n",
    "    saveAsTable(f'{username}_ghraw_db.ghactivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'SHOW PARTITIONS {username}_ghraw_db.ghactivity').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls -R /user/${USER}/warehouse/${USER}_ghraw_db.db/ghactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'''\n",
    "    SELECT substring(created_at, 1, 10) AS created_dt, count(1)\n",
    "    FROM {username}_raw.ghactivity\n",
    "    GROUP BY created_dt\n",
    "    ORDER BY created_dt\n",
    "'''). \\\n",
    "    show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
